# Analysis Scripts Documentation

ë³¸ ë¬¸ì„œëŠ” `/script` ë””ë ‰í† ë¦¬ì— ìœ„ì¹˜í•œ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸ ìŠ¤í¬ë¦½íŠ¸ë“¤ì— ëŒ€í•œ ìƒì„¸í•œ ì„¤ëª…ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ ìŠ¤í¬ë¦½íŠ¸ë“¤ì€ í—¤ë„ë‹‰ ê°€ê²© ëª¨ë¸ë§ì„ í†µí•œ ì•„íŒŒíŠ¸ ê°€ê²© ë¶„ì„ê³¼ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” ì™„ì „í•œ ë¶„ì„ ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.

## ğŸ“ íŒŒì¼ ê°œìš”

| ìˆœì„œ | íŒŒì¼ëª… | ëª©ì  | ì£¼ìš” ê¸°ëŠ¥ |
|------|--------|------|-----------|
| 00 | `00_load_data.py` | ì›ì‹œ ë°ì´í„° ë¡œë“œ | ë ˆì´ì–´ë³„ ë°ì´í„° ë¶„ë¥˜ ë° ë¡œë“œ |
| 01 | `01_clean_merge.py` | ë°ì´í„° ì •ì œ ë° ë³‘í•© | í¼ì§€ ë§¤ì¹­, ì‹œê³µê°„ ê²°í•© |
| 02 | `02_feature_engineer.py` | í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ | íŒŒìƒ ë³€ìˆ˜ ìƒì„±, ë¡¤ë§ ì§€í‘œ |
| 03 | `03_build_panel.py` | íŒ¨ë„ ë°ì´í„° êµ¬ì¶• | ì‹œê°„-ë‹¨ì§€ ì¸ë±ìŠ¤ ì„¤ì • |
| 04 | `04_eda_qc.py` | íƒìƒ‰ì  ë¶„ì„ | í’ˆì§ˆ ê´€ë¦¬, ì‹œê°í™” |
| 05 | `05_prepare_model.py` | ëª¨ë¸ ì¤€ë¹„ | ë¶ˆí•„ìš” ë³€ìˆ˜ ì œê±° |
| 06 | `06_transform_dist.py` | ë¶„í¬ ë³€í™˜ | Yeo-Johnson ë³€í™˜ |
| 07 | `07_train_models.py` | ëª¨ë¸ í›ˆë ¨ | í—¤ë„ë‹‰ íšŒê·€, êµì°¨ê²€ì¦ |
| 08 | `08_event_study.py` | ì´ë²¤íŠ¸ ìŠ¤í„°ë”” | ê°€ê²© ìŠ¤ì¼€ì¼ ê³¡ì„  ë¶„ì„ |
| 09 | `09_predict_3rd.py` | 3ê¸° ì˜ˆì¸¡ | ë¯¸ë˜ ê°€ê²© ì˜ˆì¸¡ |
| 10 | `10_residual_analysis.py` | ì”ì°¨ ë¶„ì„ | ëª¨ë¸ ì§„ë‹¨ |

---

## ğŸ”§ ìƒì„¸ ìŠ¤í¬ë¦½íŠ¸ ë¶„ì„

### 1. `00_load_data.py` - ì›ì‹œ ë°ì´í„° ë¡œë“œ

**ëª©ì **: ë‹¤ì–‘í•œ í˜•íƒœì˜ ì›ì‹œ CSV ë°ì´í„°ë¥¼ ë¶„ì„ ëª©ì ì— ë”°ë¼ 5ê°œ ë ˆì´ì–´ë¡œ ë¶„ë¥˜í•˜ì—¬ ë¡œë“œ

**ì£¼ìš” ë¡œì§**:
```python
# ë ˆì´ì–´ êµ¬ì„± ì •ì˜
LAYER_CONFIG = {
    "A": {
        "desc": "ë‹¨ì§€ ë©”íƒ€ (1Â·2Â·3ê¸°)",
        "pattern": "ì•„íŒŒíŠ¸_ë§¤ë§¤/**/*06_24/*.csv|ì•„íŒŒíŠ¸_ë§¤ë§¤/í•œêµ­ë¶€ë™ì‚°ì›_ê³µë™ì£¼íƒ ë‹¨ì§€ ì‹ë³„ì •ë³´_ê¸°ë³¸ì •ë³´_*.csv",
        "outfile": "layer_A_complex_meta",
    },
    "B": {
        "desc": "ì‹¤ê±°ë˜ ê¸°ë¡ (1Â·2ê¸°)",
        "pattern": "transactions/*ê±°ë˜*.csv|ì•„íŒŒíŠ¸_ë§¤ë§¤/**/*ì‹¤ê±°ë˜ê°€*.csv",
        "outfile": "layer_B_transactions",
    },
    "C": {
        "desc": "ì‹œì¥ ì§€ìˆ˜Â·ê±°ì‹œ ë³€ìˆ˜",
        "pattern": "transactions/*ê°€ê²©ì§€ìˆ˜*.csv|else/*.csv",
        "outfile": "layer_C_macro_index",
    },
    "D": {
        "desc": "ê³µê¸‰ (ë¯¸ë¶„ì–‘Â·ì…ì£¼ ì˜ˆì •)",
        "pattern": "supply/*.csv",
        "outfile": "layer_D_supply",
    },
    "E": {
        "desc": "ì²­ì•½ ê²½ìŸë¥ ",
        "pattern": "competition/*.csv",
        "outfile": "layer_E_competition",
    },
}

def _load_layer(layer: str, base_dir: Path) -> pd.DataFrame:
    cfg = LAYER_CONFIG[layer]
    files = _collect_files(base_dir, cfg["pattern"])
    
    # ë ˆì´ì–´ë³„ íŠ¹ìˆ˜ ì²˜ë¦¬
    if layer == "A":
        # ì‹¤ê±°ë˜ íŒŒì¼ ì œì™¸, ë©”íƒ€ ë°ì´í„°ë§Œ
        files = [f for f in files if 'ì‹¤ê±°ë˜' not in f.name]
        
        # ë™ì  í—¤ë” ê°ì§€
        for f in files:
            header_row = 0
            with f.open('r', encoding='cp949', errors='ignore') as fin:
                for i, line in enumerate(fin):
                    if 'ë‹¨ì§€ì½”ë“œ' in line and 'ë‹¨ì§€ëª…' in line:
                        header_row = i
                        break
            
            df = pd.read_csv(f, skiprows=header_row, header=0, encoding="cp949")
            
            # ì»¬ëŸ¼ëª… í†µì¼
            for src in ["ë‹¨ì§€ëª…","complex","ë‹¨ì§€"]:
                if src in df.columns:
                    df.rename(columns={src: "complex_name"}, inplace=True)
            
            if "ë‹¨ì§€ì½”ë“œ" in df.columns:
                df.rename(columns={"ë‹¨ì§€ì½”ë“œ": "complex_id"}, inplace=True)
    
    elif layer == "B":
        # ì‹¤ê±°ë˜ ë°ì´í„° ì²˜ë¦¬
        for f in files:
            header_row = 0
            with f.open('r', encoding='cp949', errors='ignore') as fin:
                for i, line in enumerate(fin):
                    if line.lstrip().startswith('NO'):
                        header_row = i
                        break
            
            df = pd.read_csv(f, skiprows=header_row, header=0, encoding="cp949")
            
            # ì»¬ëŸ¼ëª… í†µì¼
            for src in ["ë‹¨ì§€ëª…","complex","ë‹¨ì§€"]:
                if src in df.columns:
                    df.rename(columns={src: "complex_name"}, inplace=True)
```

**í•µì‹¬ ê¸°ëŠ¥**:
- ê¸€ë¡œë¸Œ íŒ¨í„´ ê¸°ë°˜ íŒŒì¼ ìˆ˜ì§‘
- ë ˆì´ì–´ë³„ ë§ì¶¤ ì „ì²˜ë¦¬
- ë™ì  í—¤ë” ê°ì§€
- ê°•ê±´í•œ ì¸ì½”ë”© ì²˜ë¦¬
- ì»¬ëŸ¼ëª… í‘œì¤€í™”

---

### 2. `01_clean_merge.py` - ë°ì´í„° ì •ì œ ë° ë³‘í•©

**ëª©ì **: ë¡œë“œëœ ë ˆì´ì–´ë“¤ì„ ì •ì œí•˜ê³  í¼ì§€ ë§¤ì¹­ì„ í†µí•´ ë‹¨ì§€ë³„-ì‹œì ë³„ íŒ¨ë„ ë°ì´í„°ë¡œ ë³‘í•©

**ì£¼ìš” ë¡œì§**:
```python
# ë‚ ì§œ ì „ì²˜ë¦¬
def _prep_date(df, col):
    df[col] = pd.to_datetime(df[col])
    df["year_month"] = df[col].dt.to_period("M").dt.to_timestamp()
    return df

# ê³µê°„ ë§¤í•‘: ê±°ë˜ B â†’ ë‹¨ì§€ A
def normalize(x):
    if pd.notna(x):
        s = x.lower()
        s = re.sub(r"\(.*?\)", "", s)  # ê´„í˜¸ ì œê±°
        s = s.replace("ì•„íŒŒíŠ¸", "")     # 'ì•„íŒŒíŠ¸' ì œê±°
        s = re.sub(r"[\s\-\.\(\)]", "", s)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        return s
    return ""

# í¼ì§€ ë§¤ì¹­
unmapped_norm = nomap.loc[nomap["complex_id"].isna(), "complex_norm"].dropna().unique()
if len(unmapped_norm):
    # ë¸”ë¡ ê¸°ë°˜ í›„ë³´êµ° ì¶•ì†Œ
    block_map = {}
    for norm_key, cid in name_dict_norm.items():
        blk = norm_key[:2]  # ì²« 2ê¸€ìë¡œ ë¸”ë¡ ìƒì„±
        block_map.setdefault(blk, []).append(norm_key)
    
    matches = {}
    for norm_val in tqdm(unmapped_norm, desc="fuzzy matching by block"):
        blk = norm_val[:2]
        choices = block_map.get(blk, list(name_dict_norm.keys()))
        threshold = 90 if len(norm_val) > 4 else 85
        match = process.extractOne(norm_val, choices, scorer=fuzz.ratio)
        matches[norm_val] = name_dict_norm.get(match[0]) if match and match[1] >= threshold else np.nan

# Wide â†’ Long ë³€í™˜ (C ë ˆì´ì–´)
date_pattern = re.compile(r"^\d{4}(?:\.\d+)?(?:/2)?(?:ë…„)?$")
date_cols = [c for c in C_wide.columns if date_pattern.match(c)]
id_vars = [c for c in C_wide.columns if c not in date_cols]
C = C_wide.melt(id_vars=id_vars, value_vars=date_cols,
                var_name="year_month_raw", value_name="macro_index")

def parse_ym(s):
    s2 = s.replace("/2", "").replace("ë…„", "")
    return pd.to_datetime(s2, format="%Y.%m", errors="coerce")

C["year_month"] = C["year_month_raw"].apply(parse_ym)
```

**í•µì‹¬ ê¸°ëŠ¥**:
- í¼ì§€ ë§¤ì¹­ì„ í†µí•œ ë‹¨ì§€ëª… ë§¤í•‘
- ë¸”ë¡ ê¸°ë°˜ ë§¤ì¹­ ì„±ëŠ¥ ìµœì í™”
- Wide-to-Long ë°ì´í„° ë³€í™˜
- ì‹œê³µê°„ ê²°í•© (complex_id + year_month)
- ìˆ˜ë™ ë§¤í•‘ í…Œì´ë¸” ì§€ì›

---

### 3. `02_feature_engineer.py` - í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§

**ëª©ì **: í—¤ë„ë‹‰ ëª¨ë¸ë§ì„ ìœ„í•œ íŒŒìƒ ë³€ìˆ˜ ìƒì„± ë° ë¡¤ë§ ì§€í‘œ ê³„ì‚°

**ì£¼ìš” ë¡œì§**:
```python
# íŒŒìƒ ê°€ê²© ë³€ìˆ˜
df["price_per_m2"] = df["price"] / df["area_m2"]
df["ln_price"] = np.log(df["price_per_m2"].clip(lower=args.min_clip))

# ê±´ì¶• ì—°ì°¨
if {"built_year","contract_year"}.issubset(df.columns):
    df["built_age"] = df["contract_year"] - df["built_year"]
    
    # built_year ê²°ì¸¡ ì¹˜í™˜ (ë‹¨ì§€ë³„ í‰ê· )
    mean_by_cx = df.groupby("complex_id")["built_year"].transform("mean")
    df.loc[df["built_year"].isna(), "built_year"] = mean_by_cx
    df["built_age"] = df["contract_year"] - df["built_year"]

# ì§€ì—­ ë”ë¯¸ (ì‹œêµ°êµ¬ ê³ ì • íš¨ê³¼)
if args.sparse_dummies:
    dummies = pd.get_dummies(df["ì‹œêµ°êµ¬ëª…"], prefix="reg", dtype="int8", sparse=True)
else:
    dummies = pd.get_dummies(df["ì‹œêµ°êµ¬ëª…"], prefix="reg", dtype="int8")
df = pd.concat([df, dummies], axis=1)

# ê³µê¸‰ ì‡¼í¬: ë¯¸ë¶„ì–‘ 12ê°œì›” ëˆ„ì 
df.sort_values(["ì‹œêµ°êµ¬ëª…","year_month"], inplace=True)
if "unsold_units" in df.columns:
    df[f"unsold_units_{args.roll_window_supply}m"] = (
        df.groupby("ì‹œêµ°êµ¬ëª…")["unsold_units"]
          .transform(lambda s: s.fillna(0).rolling(window=args.roll_window_supply, min_periods=1).sum())
    )

# ì²­ì•½ ê³¼ì—´: ê²½ìŸë¥  3ê°œì›” MA
if "comp_rate" in df.columns:
    df[f"comp_rate_ma{args.roll_window_demand}"] = (
        df.groupby("ì‹œêµ°êµ¬ëª…")["comp_rate"]
          .transform(lambda s: s.ffill().rolling(window=args.roll_window_demand, min_periods=1).mean())
    )

# ê²°ì¸¡ ì²˜ë¦¬: ë‹¨ì§€/ì§€ì—­ ë‹¨ìœ„ í‰ê·  ëŒ€ì²´
for col in num_cols:
    if df[col].isna().any():
        df[col] = df.groupby("complex_id")[col].transform(lambda s: s.fillna(s.mean()))
        df[col] = df.groupby("ì‹œêµ°êµ¬ëª…")[col].transform(lambda s: s.fillna(s.mean()))
```

**í•µì‹¬ ê¸°ëŠ¥**:
- ë¡œê·¸ ê°€ê²© ë³€í™˜
- ê±´ì¶• ì—°ì°¨ ê³„ì‚°
- ì§€ì—­ ê³ ì • íš¨ê³¼ ë”ë¯¸
- ë¡¤ë§ ìœˆë„ìš° ì§€í‘œ
- ê³„ì¸µì  ê²°ì¸¡ê°’ ì²˜ë¦¬

---

### 4. `03_build_panel.py` - íŒ¨ë„ ë°ì´í„° êµ¬ì¶•

**ëª©ì **: í—¤ë„ë‹‰ ëª¨ë¸ë§ì„ ìœ„í•œ í‘œì¤€ íŒ¨ë„ ë°ì´í„° êµ¬ì¡° ìƒì„±

**ì£¼ìš” ë¡œì§**:
```python
# ì‹œê°„ ì‹ë³„ì ìƒì„±
df["year_month"] = pd.to_datetime(df["year_month"])
df["time_id"] = df["year_month"].dt.year * 100 + df["year_month"].dt.month

# ì‹œì  ê³ ì • íš¨ê³¼ ë”ë¯¸ ìƒì„± (ì„ íƒì‚¬í•­)
if args.add_time_dummies:
    dummies = pd.get_dummies(
        df["time_id"],
        prefix=args.time_dummy_prefix,
        dtype="int8",
        sparse=args.sparse_dummies
    )
    
    # Parquet í˜¸í™˜ì„±ì„ ìœ„í•œ dense ë³€í™˜
    if args.sparse_dummies:
        dummies = dummies.sparse.to_dense()
    
    df = pd.concat([df, dummies], axis=1)

# ë©€í‹°ì¸ë±ìŠ¤ ì„¤ì • ë° ì¤‘ë³µ ì œê±°
df.set_index(["complex_id", "year_month"], inplace=True)
df = df[~df.index.duplicated(keep="first")]
```

**í•µì‹¬ ê¸°ëŠ¥**:
- ì‹œê°„ ID ìƒì„±
- ì‹œì  ê³ ì • íš¨ê³¼ ë”ë¯¸
- ë©€í‹°ì¸ë±ìŠ¤ ì„¤ì •
- ì¤‘ë³µ ë ˆì½”ë“œ ì œê±°
- Parquet ìµœì í™”

---

### 5. `04_eda_qc.py` - íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ ë° í’ˆì§ˆ ê´€ë¦¬

**ëª©ì **: ëª¨ë¸ë§ ì „ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ë° í†µê³„ì  íŠ¹ì„± ë¶„ì„

**ì£¼ìš” ë¡œì§**:
```python
# ê²°ì¸¡ë¥  ë¶„ì„
missing_rate = df[num_cols].isna().mean().sort_values(ascending=False)
missing_rate.to_csv(OUT_DIR / "qc_missing_rate.csv")

# ì •ê·œì„± ê²€ì‚¬ (Shapiro-Wilk)
qc_norm = []
for col in ["price_per_m2", "ln_price", "built_age", "unsold_units_12m", "comp_rate_ma3"]:
    if col in df:
        series = df[col].dropna()
        # í‘œë³¸ì´ 5000ê°œ ì´ìƒì¼ ê²½ìš° ìƒ˜í”Œë§
        samp = series if len(series)<=5000 else series.sample(5000, random_state=0)
        stat, pval = stats.shapiro(samp)
        
        # Box-Cox ë³€í™˜ ëŒë‹¤ ê³„ì‚°
        if (series>0).all():
            bc_data, bc_lambda = stats.boxcox(series)
        else:
            bc_lambda = np.nan
        
        qc_norm.append({
            "feature": col,
            "n": len(series),
            "shapiro_p": pval,
            "boxcox_lambda": bc_lambda
        })

# ë‹¤ì¤‘ê³µì„ ì„± (VIF) ê³„ì‚°
X = df[num_cols].dropna().iloc[:5000]
X = X.assign(const=1.0)
for i, col in enumerate(X.columns):
    if col != "const":
        vif = variance_inflation_factor(X.values, i)
        vif_df.loc[len(vif_df)] = [col, vif]

# ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
corr = df[num_cols].corr().abs()
top_feats = missing_rate.head(20).index.tolist()
sub_corr = corr.loc[top_feats, top_feats]

plt.figure(figsize=(8,6))
plt.imshow(sub_corr, vmin=0, vmax=1, cmap="viridis")
plt.colorbar(label="|Correlation|")
plt.title("Correlation Heatmap (Top 20 by Missing Rate)")
plt.savefig(OUT_DIR / "fig_corr_heatmap.png", dpi=150)
```

**í•µì‹¬ ê¸°ëŠ¥**:
- ê²°ì¸¡ë¥  ë¶„ì„ ë° ì‹œê°í™”
- ì •ê·œì„± ê²€ì •
- Box-Cox ë³€í™˜ ëŒë‹¤ ê³„ì‚°
- VIF ë‹¤ì¤‘ê³µì„ ì„± ê²€ì‚¬
- ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ

---

### 6. `05_prepare_model.py` - ëª¨ë¸ë§ìš© ë°ì´í„° ì¤€ë¹„

**ëª©ì **: í—¤ë„ë‹‰ íšŒê·€ ëª¨ë¸ë§ì— ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ ì œê±° ë° ìµœì¢… ë°ì´í„°ì…‹ ì¤€ë¹„

**ì£¼ìš” ë¡œì§**:
```python
# ë°ì´í„° ë¡œë“œ ë° ë°±ì—…
df = pd.read_parquet(IN_PATH)
df.to_parquet(BACKUP_PATH, index=False)

# ë¶ˆí•„ìš” ì»¬ëŸ¼ ì œê±°
drop_cols = ["no"]  # ID ì»¬ëŸ¼
drop_cols += ["contract_day", "contract_ym", "contract_year", "time_id"]  # ë‚ ì§œ ê´€ë ¨
drop_cols += [c for c in df.columns if str(c).startswith("tm_")]  # ì‹œê°„ ë”ë¯¸

df_model = df.drop(columns=drop_cols, errors="ignore")
df_model.to_parquet(OUT_PATH, index=False)
```

**í•µì‹¬ ê¸°ëŠ¥**:
- ì›ë³¸ ë°ì´í„° ë°±ì—…
- ID ë° ì¤‘ë³µ ì‹œê°„ ë³€ìˆ˜ ì œê±°
- ëª¨ë¸ë§ ìµœì í™”
- ê°„ê²°í•œ í”¼ì²˜ì…‹ ìƒì„±

---

### 7. `06_transform_dist.py` - ë¶„í¬ ë³€í™˜

**ëª©ì **: í—¤ë„ë‹‰ íšŒê·€ì˜ ì •ê·œì„± ê°€ì •ì„ ë§Œì¡±í•˜ê¸° ìœ„í•œ Yeo-Johnson ë³€í™˜ ì ìš©

**ì£¼ìš” ë¡œì§**:
```python
# ë³€í™˜ ëŒ€ìƒ í”¼ì²˜
features = ["price_per_m2", "ln_price"]
lambdas = {}
bounds = {}

for feat in features:
    if feat not in df.columns:
        continue
    
    series = df[feat].dropna()
    
    # Yeo-Johnson ë³€í™˜
    yj_data, lam = yeojohnson(series)
    lambdas[feat] = lam
    col_yj = f"yj_{feat}"
    df[col_yj] = np.nan
    df.loc[series.index, col_yj] = yj_data
    
    # ìœˆì €ë¼ì´ì¦ˆ (1% í•˜/ìƒìœ„ ì ˆë‹¨)
    lower = np.nanpercentile(df[col_yj], 1)
    upper = np.nanpercentile(df[col_yj], 99)
    bounds[feat] = (lower, upper)
    col_w = f"{col_yj}_w"
    df[col_w] = df[col_yj].clip(lower, upper)
    
    # ì§„ë‹¨ í”Œë¡¯ ìƒì„±
    for col in [feat, col_yj, col_w]:
        data = df[col].dropna()
        
        # íˆìŠ¤í† ê·¸ë¨
        plt.figure(figsize=(6,4))
        plt.hist(data, bins=50)
        plt.title(f"Histogram of {col}")
        plt.savefig(PLOT_DIR / f"fig_hist_{col}.png", dpi=150)
        plt.close()
        
        # QQ-í”Œë¡¯
        plt.figure(figsize=(6,6))
        sm.qqplot(data, line="45", fit=True)
        plt.title(f"QQ-plot of {col}")
        plt.savefig(PLOT_DIR / f"fig_qq_{col}.png", dpi=150)
        plt.close()
```

**í•µì‹¬ ê¸°ëŠ¥**:
- Yeo-Johnson ë³€í™˜
- ì´ìƒì¹˜ ìœˆì €ë¼ì´ì§•
- ë³€í™˜ ì „í›„ ì§„ë‹¨ í”Œë¡¯
- ëŒë‹¤ íŒŒë¼ë¯¸í„° ì €ì¥

---

### 8. `07_train_models.py` - í—¤ë„ë‹‰ íšŒê·€ ëª¨ë¸ í›ˆë ¨

**ëª©ì **: êµì°¨ê²€ì¦ì„ í†µí•œ í—¤ë„ë‹‰ ê°€ê²© ëª¨ë¸ í›ˆë ¨ ë° ì„±ëŠ¥ í‰ê°€

**ì£¼ìš” ë¡œì§**:
```python
# íŠ¹ì„± ë° íƒ€ê²Ÿ ì„¤ì •
target = args.target  # "ln_price"
numeric_cols = [c for c in df.columns if is_numeric_dtype(df[c])]
features = [c for c in numeric_cols if c != target]
X = df[features]
y = df[target]

# êµì°¨ê²€ì¦ ì„¤ì •
kf = KFold(n_splits=args.n_folds, shuffle=True, random_state=0)
metrics = []
coef_list = []
y_true_all = []
y_pred_all = []

# CV ìˆ˜í–‰
for fold, (train_idx, test_idx) in enumerate(kf.split(X), start=1):
    X_train = X.iloc[train_idx]
    X_test = X.iloc[test_idx]
    y_train = y.iloc[train_idx]
    y_test = y.iloc[test_idx]
    
    # ìƒìˆ˜í•­ ì¶”ê°€
    X_train_sm = sm.add_constant(X_train)
    X_test_sm = sm.add_constant(X_test)
    
    # í—¤ë„ë‹‰ íšŒê·€ (ê°•ê±´ í‘œì¤€ì˜¤ì°¨)
    model = sm.OLS(y_train, X_train_sm).fit(cov_type='HC3')
    
    # ì˜ˆì¸¡ ë° í‰ê°€
    y_pred = model.predict(X_test_sm)
    r2 = r2_score(y_test, y_pred)
    
    # ì›ë˜ ê°€ê²© ê³µê°„ì—ì„œ MAPE ê³„ì‚°
    true_price = np.exp(y_test)
    pred_price = np.exp(y_pred)
    mape = mean_absolute_percentage_error(true_price, pred_price)
    
    metrics.append({"fold": fold, "r2": r2, "mape": mape})
    
    # íšŒê·€ê³„ìˆ˜ ì €ì¥
    params = model.params.reset_index()
    params.columns = ['feature', 'coef']
    params['fold'] = fold
    coef_list.append(params)

# ê²°ê³¼ ì €ì¥ ë° ì‹œê°í™”
metrics_df = pd.DataFrame(metrics)
coef_df = pd.concat(coef_list, ignore_index=True)
coef_mean_df = coef_df.groupby('feature')['coef'].mean().reset_index()

# íšŒê·€ê³„ìˆ˜ ì‹œê°í™”
coef_mean = coef_df.groupby('feature')['coef'].mean().abs().sort_values(ascending=False).head(args.top_coef)
plt.figure(figsize=(8,6))
coef_mean.sort_values().plot(kind='barh')
plt.title('Top Regression Coefficients (abs mean)')
plt.savefig(OUT_DIR/"fig_coef.png", dpi=150)

# ì˜ˆì¸¡ vs ì‹¤ì œ ì‹œê°í™”
plt.figure(figsize=(6,6))
plt.scatter(y_true_all, y_pred_all, alpha=0.3)
lims = [min(min(y_true_all), min(y_pred_all)), max(max(y_true_all), max(y_pred_all))]
plt.plot(lims, lims, 'k--')
plt.xlabel('True Price_per_m2')
plt.ylabel('Predicted Price_per_m2')
plt.title('Predicted vs True')
plt.savefig(OUT_DIR/"fig_pred_vs_true.png", dpi=150)
```

**í•µì‹¬ ê¸°ëŠ¥**:
- K-Fold êµì°¨ê²€ì¦
- ê°•ê±´ í‘œì¤€ì˜¤ì°¨ (HC3)
- RÂ² ë° MAPE í‰ê°€
- íšŒê·€ê³„ìˆ˜ í‰ê· í™”
- ì˜ˆì¸¡ ì„±ëŠ¥ ì‹œê°í™”

---

### 9. `08_event_study.py` - ì´ë²¤íŠ¸ ìŠ¤í„°ë”” ë¶„ì„

**ëª©ì **: ë‹¨ì§€ë³„ ì…ì£¼ ì‹œì ì„ ê¸°ì¤€ìœ¼ë¡œ í•œ ê°€ê²© ë³€í™” íŒ¨í„´(ìŠ¤ì¼€ì¼ ê³¡ì„ ) ë¶„ì„

**ì£¼ìš” ë¡œì§**:
```python
# ë°ì´í„° ë¡œë“œ ë° íƒ€ì… í†µì¼
df = pd.read_parquet(args.panel_feat)
df['complex_id'] = df['complex_id'].astype(str).str.replace(r"\.0$", "", regex=True)
meta = pd.read_pickle(args.meta_a)
meta['complex_id'] = meta['complex_id'].astype(str)

# ì˜ˆìƒ ì…ì£¼ì¼ íŒŒì‹±
date_cols = [c for c in meta.columns if 'ìŠ¹ì¸' in c or 'ì…ì£¼' in c]
exp_col = date_cols[0]
meta['expected_date'] = pd.to_datetime(meta[exp_col], errors='coerce')
meta['expected_ym'] = meta['expected_date'].dt.to_period('M').dt.to_timestamp()

# íŒ¨ë„ê³¼ ë©”íƒ€ ë³‘í•©
df = df.merge(meta[['complex_id','expected_ym']], on='complex_id', how='left')

# tau ê³„ì‚° (ì…ì£¼ ì‹œì  ëŒ€ë¹„ ìƒëŒ€ì  ì‹œì )
df['year_month'] = pd.to_datetime(df['year_month'])
df['tau'] = (df['year_month'].dt.year*12 + df['year_month'].dt.month) - \
            (df['expected_ym'].dt.year*12 + df['expected_ym'].dt.month)

# tau ë²”ìœ„ í•„í„°ë§
df = df[(df['tau'] >= args.tau_min) & (df['tau'] <= args.tau_max)]

# tauë³„ í‰ê·  ê°€ê²© ì§‘ê³„
agg = df.groupby('tau')['price_per_m2'].mean().reset_index(name='raw_mean_price')

# LOWESS ìŠ¤ë¬´ë”©
tau = agg['tau'].values
raw = agg['raw_mean_price'].values
smoothed = lowess(raw, tau, frac=args.frac, return_sorted=True)
agg['smoothed_price'] = np.interp(tau, smoothed[:,0], smoothed[:,1])

# ì‹œê°í™”
plt.figure(figsize=(8,4))
plt.plot(agg['tau'], agg['raw_mean_price'], color='gray', label='Raw mean')
plt.plot(agg['tau'], agg['smoothed_price'], color='red', label='LOWESS smoothed')
plt.xlabel('ì‚¬ê±´ ì‹œì (tau ê°œì›”)')
plt.ylabel('í‰ê·  í‰ë‹¹ ê°€ê²©')
plt.title('ì‚¬ê±´ ì—°êµ¬: ê°€ê²© vs tau')
plt.legend()
plt.savefig(args.output_fig, dpi=150)
```

**í•µì‹¬ ê¸°ëŠ¥**:
- ìƒëŒ€ì  ì‹œì (tau) ê³„ì‚°
- ì´ë²¤íŠ¸ ìŠ¤í„°ë”” ì§‘ê³„
- LOWESS ìŠ¤ë¬´ë”©
- ê°€ê²© ìŠ¤ì¼€ì¼ ê³¡ì„  ìƒì„±

---

### 10. `09_predict_3rd.py` - 3ê¸° ê°€ê²© ì˜ˆì¸¡

**ëª©ì **: í—¤ë„ë‹‰ ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ ê³¡ì„ ì„ ê²°í•©í•˜ì—¬ 3ê¸° ë‹¨ì§€ì˜ ë¯¸ë˜ ê°€ê²© ì˜ˆì¸¡

**ì£¼ìš” ë¡œì§**:
```python
# ë°ì´í„° ë¡œë“œ
panel_trans = pd.read_parquet(args.panel)  # ë³€í™˜ëœ íŒ¨ë„
panel_base = pd.read_parquet("output/panel_panel.parquet")  # ì¸ë±ìŠ¤ ì •ë³´
panel_base = panel_base.reset_index()

# ì¸ë±ìŠ¤ ê²°í•©
panel = pd.concat([panel_base[['complex_id','year_month']], 
                   panel_trans.reset_index(drop=True)], axis=1)

# ë©”íƒ€ ë° íšŒê·€ê³„ìˆ˜ ë¡œë“œ
meta = pd.read_pickle(args.meta_a)
coef_df = pd.read_csv(args.model_coef)
scale_df = pd.read_csv(args.scale_curve)

# íšŒê·€ê³„ìˆ˜ ë° ìŠ¤ì¼€ì¼ ê³¡ì„  ì¤€ë¹„
coef_series = coef_df.set_index('feature')['coef']
scale_map = scale_df.set_index('tau')['smoothed_price'].to_dict()
f0 = scale_map.get(0, 1.0)  # ê¸°ì¤€ì  (tau=0)

# ì¶œì‹œ ì‹œì  ë©”íƒ€ ì²˜ë¦¬
meta['launch_date'] = pd.to_datetime(meta[args.launch_date_col], errors='coerce')
meta['launch_ym'] = meta['launch_date'].dt.to_period('M').dt.to_timestamp()

# ì¶œì‹œ ì‹œì  íŒ¨ë„ í”¼ì²˜ ë³‘í•©
df = panel.merge(meta[['complex_id','complex_name','launch_ym']], on='complex_id', how='inner')
df_launch = df[df['year_month'] == df['launch_ym']].copy()

# ê¸°ë³¸ ë¡œê·¸ ê°€ê²© ì˜ˆì¸¡
feature_cols = [c for c in coef_series.index if c != 'const']
X = df_launch[feature_cols]
X_sm = sm.add_constant(X)
base_ln = X_sm.dot(coef_series)
df_launch['base_price'] = np.exp(base_ln)

# í–¥í›„ tauë³„ ê°€ê²© ì˜ˆì¸¡
taus = [int(t) for t in args.tau_list.split(',')]
for tau in taus:
    f_tau = scale_map.get(tau, np.nan)
    df_launch[f'price_tau{tau}'] = df_launch['base_price'] * (f_tau / f0)

# ê²°ê³¼ ì €ì¥ ë° ì‹œê°í™”
out_cols = ['complex_id','complex_name','launch_ym','base_price'] + \
           [f'price_tau{t}' for t in taus]
df_launch[out_cols].to_csv(args.output_csv, index=False)

# tauë³„ í‰ê·  ê°€ê²© ì‹œê°í™”
mean_prices = df_launch[['base_price'] + [f'price_tau{t}' for t in taus]].mean()
plt.figure(figsize=(6,4))
mean_prices.plot(kind='bar')
plt.ylabel('Predicted price per m2')
plt.title('Predicted price at launch and future tau')
plt.savefig(args.output_fig, dpi=150)
```

**í•µì‹¬ ê¸°ëŠ¥**:
- í—¤ë„ë‹‰ ëª¨ë¸ ê¸°ë³¸ ì˜ˆì¸¡
- ìŠ¤ì¼€ì¼ ê³¡ì„  ì¡°ì •
- ë‹¤ì¤‘ ì‹œì  ì˜ˆì¸¡
- ê²°ê³¼ ì‹œê°í™”

---

### 11. `10_residual_analysis.py` - ì”ì°¨ ë¶„ì„

**ëª©ì **: í—¤ë„ë‹‰ ëª¨ë¸ì˜ ì”ì°¨ íŒ¨í„´ ë¶„ì„ì„ í†µí•œ ëª¨ë¸ ì§„ë‹¨ ë° ê°œì„ ì  ë„ì¶œ

**ì£¼ìš” ë¡œì§**:
```python
# ë°ì´í„° ë° íšŒê·€ê³„ìˆ˜ ë¡œë“œ
df = pd.read_parquet(PANEL_PATH)
coef_df = pd.read_csv(COEF_PATH)
coef_series = coef_df.set_index('feature')['coef']

# í”¼ì²˜ í–‰ë ¬ ì¤€ë¹„
target = 'ln_price'
feature_cols = [f for f in coef_series.index if f != 'const' and f in df.columns]
X = df[feature_cols]
X_sm = sm.add_constant(X)

# ì˜ˆì¸¡ê°’ ê³„ì‚°
y_true = df[target]
X_sm = X_sm.reindex(columns=coef_series.index, fill_value=0)  # ëˆ„ë½ ì»¬ëŸ¼ ì²˜ë¦¬
y_pred = X_sm.dot(coef_series)

# ì”ì°¨ ê³„ì‚°
df_res = pd.DataFrame({
    'year_month': pd.to_datetime(df['year_month']),
    'residual': y_true - y_pred
})

# ì›”ë³„ í‰ê·  ì”ì°¨ ë° ì´ë™í‰ê· 
res_ts = df_res.groupby('year_month')['residual'].mean().reset_index()
res_ts['residual_roll_mean'] = res_ts['residual'].rolling(
    window=args.rolling_window, min_periods=1).mean()

# ì”ì°¨ íˆìŠ¤í† ê·¸ë¨
plt.figure(figsize=(6,4))
plt.hist(df_res['residual'].dropna(), bins=50, color='skyblue', edgecolor='k')
plt.title('Histogram of Residuals')
plt.xlabel('Residual (log price)')
plt.ylabel('Count')
plt.savefig(OUT_DIR / 'fig_residual_hist.png', dpi=150)

# ì”ì°¨ QQ-í”Œë¡¯
gg = sm.qqplot(df_res['residual'].dropna(), line='45', fit=True)
gg.figure.set_size_inches(6,6)
gg.figure.savefig(OUT_DIR / 'fig_residual_qq.png', dpi=150)

# ì”ì°¨ ì‹œê³„ì—´ ì¶”ì„¸
plt.figure(figsize=(8,4))
plt.plot(res_ts['year_month'], res_ts['residual'], label='Mean Residual', alpha=0.6)
plt.plot(res_ts['year_month'], res_ts['residual_roll_mean'], 
         label=f'{args.rolling_window}-month Rolling Mean', color='red')
plt.xlabel('Year-Month')
plt.ylabel('Mean Residual (log price)')
plt.title('Residual Trend Over Time')
plt.legend()
plt.savefig(OUT_DIR / 'fig_residual_trend.png', dpi=150)
```

**í•µì‹¬ ê¸°ëŠ¥**:
- ì”ì°¨ ê³„ì‚° ë° ì‹œê³„ì—´ ì§‘ê³„
- ì”ì°¨ ë¶„í¬ ì‹œê°í™”
- QQ-í”Œë¡¯ ì •ê·œì„± ê²€ì •
- ì‹œê°„ ì¶”ì„¸ ë¶„ì„
- ì´ë™í‰ê·  ìŠ¤ë¬´ë”©

---

## ğŸ”„ ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸

```mermaid
graph TD
    A[00_load_data.py] --> B[01_clean_merge.py]
    B --> C[02_feature_engineer.py]
    C --> D[03_build_panel.py]
    D --> E[04_eda_qc.py]
    E --> F[05_prepare_model.py]
    F --> G[06_transform_dist.py]
    G --> H[07_train_models.py]
    H --> I[08_event_study.py]
    I --> J[09_predict_3rd.py]
    H --> K[10_residual_analysis.py]
    
    A --> A1[Layer A: ë‹¨ì§€ ë©”íƒ€]
    A --> A2[Layer B: ì‹¤ê±°ë˜]
    A --> A3[Layer C: ê±°ì‹œì§€í‘œ]
    A --> A4[Layer D: ê³µê¸‰]
    A --> A5[Layer E: ì²­ì•½ê²½ìŸë¥ ]
    
    B --> B1[í¼ì§€ ë§¤ì¹­]
    B --> B2[ì‹œê³µê°„ ê²°í•©]
    
    C --> C1[íŒŒìƒë³€ìˆ˜]
    C --> C2[ë¡¤ë§ì§€í‘œ]
    
    H --> H1[í—¤ë„ë‹‰ íšŒê·€]
    H --> H2[êµì°¨ê²€ì¦]
    
    I --> I1[ìŠ¤ì¼€ì¼ ê³¡ì„ ]
    
    J --> J1[3ê¸° ì˜ˆì¸¡]
```

## ğŸ“Š ì£¼ìš” ì‚°ì¶œë¬¼

### 1. ë°ì´í„° ì‚°ì¶œë¬¼
- `layer_A_complex_meta.pickle`: ë‹¨ì§€ ë©”íƒ€ë°ì´í„°
- `layer_B_transactions.pickle`: ì‹¤ê±°ë˜ ë°ì´í„°
- `panel_clean.parquet`: ì •ì œëœ íŒ¨ë„ ë°ì´í„°
- `panel_feat.parquet`: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ ë°ì´í„°
- `panel_model_transformed.parquet`: ë³€í™˜ëœ ëª¨ë¸ë§ ë°ì´í„°

### 2. ëª¨ë¸ ì‚°ì¶œë¬¼
- `cv_metrics.csv`: êµì°¨ê²€ì¦ ì„±ëŠ¥ ì§€í‘œ
- `coef_mean.csv`: í‰ê·  íšŒê·€ê³„ìˆ˜
- `scale_curve.csv`: ê°€ê²© ìŠ¤ì¼€ì¼ ê³¡ì„ 
- `pred_3rd.csv`: 3ê¸° ê°€ê²© ì˜ˆì¸¡ ê²°ê³¼

### 3. ì§„ë‹¨ ì‚°ì¶œë¬¼
- `qc_missing_rate.csv`: ê²°ì¸¡ë¥  ë¶„ì„
- `qc_shapiro_boxcox.csv`: ì •ê·œì„± ê²€ì • ê²°ê³¼
- `qc_vif.csv`: ë‹¤ì¤‘ê³µì„ ì„± ë¶„ì„
- `residual_timeseries.csv`: ì”ì°¨ ì‹œê³„ì—´

### 4. ì‹œê°í™” ì‚°ì¶œë¬¼
- `fig_coef.png`: íšŒê·€ê³„ìˆ˜ ì‹œê°í™”
- `fig_pred_vs_true.png`: ì˜ˆì¸¡ vs ì‹¤ì œ
- `fig_scale_curve.png`: ìŠ¤ì¼€ì¼ ê³¡ì„ 
- `fig_residual_*.png`: ì”ì°¨ ì§„ë‹¨ í”Œë¡¯

## ğŸ›  ê¸°ìˆ ì  íŠ¹ì§•

### 1. ê°•ê±´ì„± (Robustness)
- ë‹¤ì–‘í•œ ì¸ì½”ë”© ìë™ ì²˜ë¦¬
- ê²°ì¸¡ê°’ ê³„ì¸µì  ì²˜ë¦¬
- ì´ìƒì¹˜ ìœˆì €ë¼ì´ì§•
- ê°•ê±´ í‘œì¤€ì˜¤ì°¨ ì‚¬ìš©

### 2. í™•ì¥ì„± (Scalability)
- ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì•Œê³ ë¦¬ì¦˜
- ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›
- ëŒ€ìš©ëŸ‰ ë°ì´í„° ìµœì í™”

### 3. ì¬í˜„ì„± (Reproducibility)
- ì‹œë“œ ê³ ì •
- íŒŒë¼ë¯¸í„° ì €ì¥
- ë²„ì „ ê´€ë¦¬
- ë¡œê·¸ ê¸°ë¡

### 4. ëª¨ë“ˆì„± (Modularity)
- ë‹¨ê³„ë³„ ë¶„ë¦¬
- CLI ì¸í„°í˜ì´ìŠ¤
- ì„¤ì • íŒŒì¼ ì§€ì›
- ë…ë¦½ì  ì‹¤í–‰ ê°€ëŠ¥

## ğŸ¯ í—¤ë„ë‹‰ ëª¨ë¸ë§ íŠ¹ì§•

### 1. ëª¨ë¸ êµ¬ì¡°
```
ln(price_per_m2) = Î²â‚€ + Î²â‚Â·area_m2 + Î²â‚‚Â·built_age + Î²â‚ƒÂ·region_dummies + 
                   Î²â‚„Â·time_dummies + Î²â‚…Â·supply_shock + Î²â‚†Â·demand_heat + Îµ
```

### 2. ê³ ì •íš¨ê³¼
- **ì§€ì—­ ê³ ì •íš¨ê³¼**: ì‹œêµ°êµ¬ë³„ ë”ë¯¸ ë³€ìˆ˜
- **ì‹œê°„ ê³ ì •íš¨ê³¼**: ì›”ë³„ ë”ë¯¸ ë³€ìˆ˜ (ì„ íƒì‚¬í•­)
- **ë‹¨ì§€ ê³ ì •íš¨ê³¼**: íŒ¨ë„ êµ¬ì¡°ë¥¼ í†µí•œ ìë™ ì²˜ë¦¬

### 3. ë™ì  ìš”ì¸
- **ê³µê¸‰ ì‡¼í¬**: 12ê°œì›” ëˆ„ì  ë¯¸ë¶„ì–‘
- **ìˆ˜ìš” ê³¼ì—´**: 3ê°œì›” í‰ê·  ì²­ì•½ê²½ìŸë¥ 
- **ê±´ì¶• ì—°ì°¨**: ë™ì  ë…¸í›„í™” íš¨ê³¼

### 4. ì´ë²¤íŠ¸ ìŠ¤í„°ë””
- **tau**: ì…ì£¼ ì‹œì  ëŒ€ë¹„ ìƒëŒ€ì  ì›”ìˆ˜
- **ìŠ¤ì¼€ì¼ ê³¡ì„ **: LOWESS ìŠ¤ë¬´ë”©ëœ ê°€ê²© íŒ¨í„´
- **ì˜ˆì¸¡ ì¡°ì •**: ê¸°ë³¸ í—¤ë„ë‹‰ ê°€ê²© Ã— ìŠ¤ì¼€ì¼ íŒ©í„°